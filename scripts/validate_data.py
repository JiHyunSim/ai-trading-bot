#!/usr/bin/env python3
"""
AI Trading Bot - ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import sys
import argparse
from datetime import datetime, timedelta
from typing import Dict, List
import asyncpg
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class DataValidator:
    """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.db_pool = None
        
        # íƒ€ì„í”„ë ˆì„ë³„ ê°„ê²© (ë°€ë¦¬ì´ˆ)
        self.timeframe_intervals = {
            "1m": 60 * 1000,
            "5m": 5 * 60 * 1000,
            "15m": 15 * 60 * 1000,
            "1h": 60 * 60 * 1000,
            "4h": 4 * 60 * 60 * 1000,
            "1d": 24 * 60 * 60 * 1000
        }
    
    async def initialize(self):
        """ì´ˆê¸°í™”"""
        self.db_pool = await asyncpg.create_pool(
            host='localhost',
            port=5432,
            database='trading_bot',
            user='trading_bot',
            password='trading_bot_password',
            min_size=2,
            max_size=5
        )
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.db_pool:
            await self.db_pool.close()
    
    async def check_data_integrity(self, symbol: str, timeframes: List[str], hours_back: int = 24) -> Dict:
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""
        
        results = {
            'summary': {},
            'timeframes': {},
            'issues': []
        }
        
        async with self.db_pool.acquire() as conn:
            end_time = datetime.now()
            start_time = end_time - timedelta(hours=hours_back)
            
            start_ts = int(start_time.timestamp() * 1000)
            end_ts = int(end_time.timestamp() * 1000)
            
            print(f"ğŸ” Validating data integrity for {symbol}")
            print(f"ğŸ“… Period: {start_time.strftime('%Y-%m-%d %H:%M')} ~ {end_time.strftime('%Y-%m-%d %H:%M')}")
            print("=" * 60)
            
            total_records = 0
            total_expected = 0
            total_gaps = 0
            total_duplicates = 0
            
            for timeframe in timeframes:
                print(f"\nğŸ“Š Analyzing {timeframe}...")
                
                # ê¸°ë³¸ í†µê³„
                stats = await conn.fetchrow("""
                    SELECT 
                        COUNT(*) as total_records,
                        COUNT(DISTINCT timestamp_ms) as unique_records,
                        MIN(timestamp_ms) as first_timestamp,
                        MAX(timestamp_ms) as last_timestamp,
                        MIN(created_at) as first_created,
                        MAX(created_at) as last_created
                    FROM trading.candlesticks 
                    WHERE symbol = $1 AND timeframe = $2 
                    AND timestamp_ms BETWEEN $3 AND $4
                """, symbol, timeframe, start_ts, end_ts)
                
                if not stats['total_records']:
                    print(f"  âŒ No data found")
                    results['timeframes'][timeframe] = {
                        'status': 'no_data',
                        'records': 0,
                        'expected': 0,
                        'gaps': 0,
                        'duplicates': 0
                    }
                    continue
                
                # ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜ ê³„ì‚°
                interval_ms = self.timeframe_intervals[timeframe]
                expected_records = (end_ts - start_ts) // interval_ms
                
                # ì¤‘ë³µ ê²€ì‚¬
                duplicates = stats['total_records'] - stats['unique_records']
                
                # ì—°ì†ì„± ê²€ì‚¬ (gaps)
                timestamps = await conn.fetch("""
                    SELECT timestamp_ms 
                    FROM trading.candlesticks 
                    WHERE symbol = $1 AND timeframe = $2 
                    AND timestamp_ms BETWEEN $3 AND $4
                    ORDER BY timestamp_ms
                """, symbol, timeframe, start_ts, end_ts)
                
                gaps = 0
                if len(timestamps) > 1:
                    for i in range(1, len(timestamps)):
                        expected_next = timestamps[i-1]['timestamp_ms'] + interval_ms
                        actual_next = timestamps[i]['timestamp_ms']
                        if actual_next > expected_next:
                            gap_size = (actual_next - expected_next) // interval_ms
                            gaps += gap_size
                
                # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
                quality_issues = await conn.fetch("""
                    SELECT 
                        COUNT(*) FILTER (WHERE open_price <= 0) as invalid_open,
                        COUNT(*) FILTER (WHERE high_price <= 0) as invalid_high,
                        COUNT(*) FILTER (WHERE low_price <= 0) as invalid_low,
                        COUNT(*) FILTER (WHERE close_price <= 0) as invalid_close,
                        COUNT(*) FILTER (WHERE volume < 0) as invalid_volume,
                        COUNT(*) FILTER (WHERE high_price < low_price) as invalid_high_low,
                        COUNT(*) FILTER (WHERE high_price < open_price OR high_price < close_price) as invalid_high_range,
                        COUNT(*) FILTER (WHERE low_price > open_price OR low_price > close_price) as invalid_low_range
                    FROM trading.candlesticks 
                    WHERE symbol = $1 AND timeframe = $2 
                    AND timestamp_ms BETWEEN $3 AND $4
                """, symbol, timeframe, start_ts, end_ts)
                
                quality = quality_issues[0]
                
                # ê²°ê³¼ ì¶œë ¥
                completeness = (stats['unique_records'] / expected_records * 100) if expected_records > 0 else 0
                
                print(f"  ğŸ“ˆ Records: {stats['total_records']:,} (unique: {stats['unique_records']:,})")
                print(f"  ğŸ“Š Expected: {expected_records:,} ({completeness:.1f}% complete)")
                print(f"  ğŸ”„ Duplicates: {duplicates:,}")
                print(f"  âš ï¸  Gaps: {gaps:,}")
                
                if stats['first_timestamp'] and stats['last_timestamp']:
                    first_time = datetime.fromtimestamp(stats['first_timestamp'] / 1000)
                    last_time = datetime.fromtimestamp(stats['last_timestamp'] / 1000)
                    print(f"  ğŸ“… Data range: {first_time.strftime('%m-%d %H:%M')} ~ {last_time.strftime('%m-%d %H:%M')}")
                
                # í’ˆì§ˆ ì´ìŠˆ ì²´í¬
                quality_issues_found = []
                for field, count in quality.items():
                    if count > 0:
                        quality_issues_found.append(f"{field}: {count}")
                
                if quality_issues_found:
                    print(f"  âŒ Quality issues: {', '.join(quality_issues_found)}")
                else:
                    print(f"  âœ… Data quality: OK")
                
                # ìƒíƒœ íŒì •
                status = 'ok'
                if completeness < 95:
                    status = 'incomplete'
                if duplicates > 0:
                    status = 'duplicates'
                if quality_issues_found:
                    status = 'quality_issues'
                if stats['total_records'] == 0:
                    status = 'no_data'
                
                results['timeframes'][timeframe] = {
                    'status': status,
                    'records': stats['total_records'],
                    'unique_records': stats['unique_records'],
                    'expected': expected_records,
                    'completeness': completeness,
                    'gaps': gaps,
                    'duplicates': duplicates,
                    'quality_issues': dict(quality),
                    'first_timestamp': stats['first_timestamp'],
                    'last_timestamp': stats['last_timestamp']
                }
                
                total_records += stats['total_records']
                total_expected += expected_records
                total_gaps += gaps
                total_duplicates += duplicates
            
            # ì „ì²´ ìš”ì•½
            overall_completeness = (total_records / total_expected * 100) if total_expected > 0 else 0
            
            print(f"\nğŸ“Š Overall Summary")
            print("=" * 30)
            print(f"Total records: {total_records:,}")
            print(f"Expected records: {total_expected:,}")
            print(f"Completeness: {overall_completeness:.1f}%")
            print(f"Total duplicates: {total_duplicates:,}")
            print(f"Total gaps: {total_gaps:,}")
            
            results['summary'] = {
                'total_records': total_records,
                'total_expected': total_expected,
                'completeness': overall_completeness,
                'total_duplicates': total_duplicates,
                'total_gaps': total_gaps
            }
            
            # ê¶Œì¥ì‚¬í•­
            print(f"\nğŸ’¡ Recommendations")
            print("=" * 30)
            
            if total_duplicates > 0:
                print(f"ğŸ”§ Run deduplication: python scripts/deduplicate_data.py")
                results['issues'].append('duplicates_found')
            
            if total_gaps > 0:
                print(f"ğŸ“¥ Fill gaps: python scripts/fill_gaps.py --hours {hours_back}")
                results['issues'].append('gaps_found')
            
            if overall_completeness < 95:
                print(f"âš ï¸  Low completeness: Consider extending data collection period")
                results['issues'].append('low_completeness')
            
            if not results['issues']:
                print("âœ… Data integrity is good!")
        
        return results

async def main():
    parser = argparse.ArgumentParser(description='AI Trading Bot Data Validation Tool')
    parser.add_argument('--symbol', default='BTC-USDT-SWAP', help='Trading symbol')
    parser.add_argument('--timeframes', default='5m,15m,1h,4h,1d', help='Comma-separated timeframes')
    parser.add_argument('--hours', type=int, default=24, help='Hours to validate')
    parser.add_argument('--json', action='store_true', help='Output results as JSON')
    
    args = parser.parse_args()
    
    timeframes = [tf.strip() for tf in args.timeframes.split(',')]
    
    validator = DataValidator()
    
    try:
        await validator.initialize()
        
        results = await validator.check_data_integrity(
            symbol=args.symbol,
            timeframes=timeframes,
            hours_back=args.hours
        )
        
        if args.json:
            import json
            print(json.dumps(results, indent=2))
        
        # ì¢…ë£Œ ì½”ë“œ ê²°ì •
        if results['issues']:
            return 1  # ì´ìŠˆê°€ ìˆìŒ
        else:
            return 0  # ì •ìƒ
    
    except Exception as e:
        print(f"âŒ Error: {e}")
        return 1
    finally:
        await validator.close()

if __name__ == "__main__":
    sys.exit(asyncio.run(main()))